
import pytest
import time
import os
import uuid
from pathlib import Path
from unittest.mock import MagicMock, patch
from app.pipeline.orchestrator import PipelineOrchestrator

def log(msg):
    print(f"\n[INTEGRATION] {msg}")

@pytest.mark.integration
class TestEndToEndIntegration:
    """
    End-to-End Integration Tests for Manuscript Formatter Pipeline.
    Simplified version for stdout capture.
    """
    
    @pytest.fixture(scope="class")
    def samples_dir(self):
        return Path("samples")
    
    @pytest.fixture
    def mock_db_session(self):
        """Mock Supabase client to capture DB updates without real DB."""
        with patch("app.pipeline.orchestrator.get_supabase_client") as mock_sb:
            mock_client = MagicMock()
            mock_sb.return_value = mock_client
            
            # Mock Supabase table operations (chained method pattern)
            mock_table = MagicMock()
            mock_client.table.return_value = mock_table
            mock_table.select.return_value = mock_table
            mock_table.update.return_value = mock_table
            mock_table.insert.return_value = mock_table
            mock_table.eq.return_value = mock_table
            mock_table.match.return_value = mock_table
            mock_table.execute.return_value = MagicMock(data=[])
            
            yield mock_client

    @pytest.fixture
    def mock_ai_engines(self):
        """Mock AI engines and services to prevent real model loading/API calls."""
        with patch("app.pipeline.intelligence.reasoning_engine.get_reasoning_engine") as mock_re:
            with patch("app.pipeline.intelligence.rag_engine.get_rag_engine") as mock_rag:
                with patch("app.pipeline.intelligence.semantic_parser.get_semantic_parser") as mock_sem:
                    with patch("app.pipeline.orchestrator.GROBIDClient") as mock_grobid:
                        with patch("app.pipeline.orchestrator.DoclingClient") as mock_docling:
                            mock_re.return_value = MagicMock()
                            mock_rag.return_value = MagicMock()
                            mock_sem.return_value = MagicMock()
                            mock_grobid.return_value = MagicMock()
                            mock_docling.return_value = MagicMock()
                            yield {
                                "reasoning": mock_re.return_value,
                                "rag": mock_rag.return_value,
                                "semantic": mock_sem.return_value,
                                "grobid": mock_grobid.return_value,
                                "docling": mock_docling.return_value
                            }

    @pytest.fixture
    def orchestrator(self, mock_ai_engines):
        return PipelineOrchestrator(templates_dir="app/templates", temp_dir="temp")

    def test_samples_exist(self, samples_dir):
        """Verify sample PDFs availability."""
        pdf_files = list(samples_dir.glob("*.pdf"))
        log(f"Checking for samples in {samples_dir.absolute()}")
        assert len(pdf_files) > 0, "No sample PDFs found in 'samples/' directory."
        log(f"Found {len(pdf_files)} sample PDFs.")

    def test_end_to_end_pipeline(self, orchestrator, samples_dir, mock_db_session):
        """Run full pipeline and measure performance."""
        mock_client = mock_db_session
        pdf_files = list(samples_dir.glob("*.pdf"))[:2]
        
        if not pdf_files:
            pytest.skip("No samples to test")
            
        success_count = 0
        total_time = 0
        
        log("="*60)
        log("STARTING END-TO-END PIPELINE PERFORMANCE TEST")
        log("="*60)
        
        for i, pdf_path in enumerate(pdf_files):
            log(f"Processing {pdf_path.name}...")
            start_time = time.time()
            
            try:
                # Orchestrator run
                response = orchestrator.run_pipeline(
                    input_path=str(pdf_path.absolute()),
                    job_id=str(uuid.uuid4()),
                    template_name="IEEE"
                )
                
                duration = time.time() - start_time
                total_time += duration
                log(f"SUCCESS: {pdf_path.name} processed in {duration:.2f}s")
                log(f"Response Status: {response['status']}")
                
                assert response["status"] == "success"
                success_count += 1
            except Exception as e:
                log(f"FAILURE: {pdf_path.name} error: {str(e)}")
                raise e

        avg_time = total_time / success_count if success_count > 0 else 0
        log("="*60)
        log(f"PERFORMANCE SUMMARY: Average Time: {avg_time:.2f}s per file")
        log(f"Success Rate: {success_count}/{len(pdf_files)}")
        log("="*60)
        
        assert avg_time < 15.0, f"Average processing time {avg_time:.2f}s exceeds 15s target"
        assert success_count == len(pdf_files)

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
